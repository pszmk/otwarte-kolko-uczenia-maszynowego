{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podstawy algebry liniowej w pytorch\n",
    "- wektor\n",
    "- macierz\n",
    "- tensor\n",
    "- operacje na wektorach i macierzach oraz ich sens jako przekształcenia geometryczne\n",
    "- operacje na tensorach\n",
    "- indeksowanie\n",
    "- iloczyn skalarny a kosinus\n",
    "- obroty\n",
    "- przesunięcia\n",
    "- skalowanie\n",
    "- złożenia przekształceń\n",
    "- pojęcie bliskości wektorów\n",
    "\n",
    "## Zbiory danych\n",
    "- chmura punktów, 2d, 3d\n",
    "- regresja (np. Boston, diabetes)\n",
    "- klasyfikacja (np. Iris)\n",
    "- zadania na obrazach\n",
    "- zadania na tekście\n",
    "- zadania na dźwięku\n",
    "- zadania na wideo\n",
    "- zadania na sekwencjach\n",
    "- zadania na grafach\n",
    "- zadania na danych przestrzennych\n",
    "\n",
    "- pojecie macierzy danych\n",
    "- pojecie tensora\n",
    "- pojecie batcha\n",
    "- pojecie epoki\n",
    "- pojecie zbioru treningowego, walidacyjnego, testowego\n",
    "- pojecie straty\n",
    "- pojecie metryki\n",
    "- pojecie modelu\n",
    "- pojecie uczenia nadzorowanego\n",
    "- pojecie uczenia nienadzorowanego\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wprowadzimy podstawowe pojęcia z algebry liniowej, które są niezbędne do zrozumienia jak zachodzi przetwarzanie informacji w modelach uczenia głębokiego. Przyjżymy się tensorom oraz operacjom na tensorach, które są podstawowymi obiektami w uczeniu maszynowym.\n",
    "\n",
    "Będziemy korzystać z biblioteki pytorch, która jest jedną z najpopularniejszych bibliotek do uczenia maszynowego. Pytorch jest biblioteką do obliczeń numerycznych, która pozwala na wykonywanie operacji na tensorach na procesorze CPU lub karcie graficznej GPU. Pytorch jest bardzo podobny do numpy, ale dodatkowo pozwala na automatyczne różniczkowanie funkcji (lepiej zrozumiemy o co chodzi na następnych zajęciach), co jest niezbędne dla wielu algorytmów uczenia głębokiego.\n",
    "\n",
    "Poniżej importujemy bibliotekę pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import Tensor # dodaje wyłącznie typ Tensor do oznaczania zmiennych dla większej czytelności"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wektory\n",
    "### Czym jest wektor?\n",
    "Wektor (*eng. vector*) to uporządkowany zbiór liczb o określonej liczbie elementów czyli liczbie wymiarów (*eng. dimensions*). Można myśleć o wektorach jako współrzędnych pewnych punktów w przestrzeni. Konkretnie wektor można zapisać jako:\n",
    "$$ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_d \\end{bmatrix}$$\n",
    "gdzie $v_1, v_2, \\ldots, v_d$ to kolejne współrzędne wektora $\\mathbf{v}$. Współrzędne to najczęściej po prostu liczby rzeczywiste. W takim razie możemy pisać, że $v \\in \\mathbb{R}$ - czyli współrzędne to liczby rzeczywiste oraz $\\mathbf{v} \\in \\mathbb{R}^d$ - czyli wektor ma $d$ wymiarów.\n",
    "\n",
    "Wektory będziemy wykorzystywali do reprezentowania pojedynczych obserwacji ze zbiorów danych. Dla przykładu, jeśli weźmiemy zestaw pomiarów meteorologicznych (temperatura, wilgotność, ciśnienie) z jednego dnia, to możemy zapisać te pomiary jako wektor o trzech współrzędnych.\n",
    "\n",
    "### Jak wygląda wektor w pytorch?\n",
    "W pytorch używamy ogólnego pojęcia tensoru, o którym będzie mowa poźniej z perspektywy matematycznej, do reprezentowania wektorów. Wektor w pytorch to tensor jednowymiarowy. Możemy go zainicjalizować w następujący sposób:\n",
    "```python\n",
    "v = torch.tensor([0.5, 21, 3330])\n",
    "```\n",
    "określające konkretne współrzędne wektora. Jeżeli potraktujemy ten wektor jako pojedynczą obserwację meteorologiczną, to pierwsza współrzędna to temperatura, druga to wilgotność, a trzecia to ciśnienie. Teraz możemy już próbować przewidywać choćby aktywność mrówek mierzoną jako obszar po którym się porauszają w trakcie dnia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 1**\n",
    "Zainicjalizuj wektor o wymiarze 10 z losowymi wartościami za pomocą funkcji `torch.randn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4782, 0.7573, 0.6353, 0.4281, 0.3240])\n"
     ]
    }
   ],
   "source": [
    "# Przykład - losowy wektor 5-elementowy\n",
    "v = torch.rand(size=(5,))\n",
    "print(v)\n",
    "\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 2**\n",
    "Zainicjalizuj wektor o wymiarze 10 z losowymi wartościami za pomocą funkcji `torch.randn` i oblicz sumę jego współrzędnych korzystając z funkcji `torch.sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 3**\n",
    "Wybierz i wypisz drugi element losowego wektora wskazując ją. Następnie zastąp tę wartość wartością 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6564, 0.0864, 0.9801, 0.9817, 0.3724])\n",
      "tensor(0.6564)\n",
      "tensor([0.0000, 0.0864, 0.9801, 0.9817, 0.3724])\n"
     ]
    }
   ],
   "source": [
    "v = torch.rand(size=(5,))\n",
    "# Przykład - wybór pierwszego elementu z wektora\n",
    "print(v)\n",
    "print(v[0])\n",
    "# zamiana pierwszego elementu na 0\n",
    "v[0] = 0\n",
    "print(v)\n",
    "\n",
    "v = torch.rand(size=(5,))\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Długość wektora\n",
    "Długość wektora, czy inaczej jego norma (*eng. norm*), to nic innego jak jakoś zdefiniowana odległość od początku układu współrzędnych do punktu reprezentowanego przez wektor. Długość wektora będziemy obliczać korzystając z twierdzenia Pitagorasa:\n",
    "$$ \\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_d^2} $$\n",
    "co w bardziej znajomym dwuwymiarowym przypadku sprowadza się do:\n",
    "$$ \\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2} $$\n",
    "\n",
    "\n",
    "W pewnych przypadkach długość wektora może mieć interpretację. Dla przykładu, jeśli mamy wektor długości równej liczbie wszystkich komarów, gdzie każdy wymiar reprezentuje średnią temperaturę ciała pojedynczego komara w stopniach Kelwina to $0$ będzie oznaczało zmrożenie na suchą kość wszystkich komarów. Im bliżej do $0$ tym bliżej do takiego cudownego stanu rzeczy - biorąc pod uwagę poważny udział komarów w łańcuchu pokarmowym być może i zgubny, choć mrożone komary również pewnie dają się zjeść.\n",
    "\n",
    "W pytorch możemy obliczyć długość wektora korzystając z funkcji `torch.norm`:\n",
    "```python\n",
    "v = torch.tensor([0.5, 21, 3330])\n",
    "torch.norm(v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 4**\n",
    "Zainicjalizuj wektor o współrzędnych $[1, 2, 3, 4, 5]$ typu `torch.float32` używając argumentu `dtype` i oblicz jego długość dla parametrów `p=1` i `p=2` funkcji `torch.norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5394)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO - begin\n",
    "v = torch.tensor(data=..., dtype=...)\n",
    "# TODO - end\n",
    "torch.norm(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 5**\n",
    "Rozwiąż równanie $\\left\\| \\begin{bmatrix} \\mathbf{v} \\\\ x \\end{bmatrix} \\right\\| = \\left\\| \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_2 \\\\ x \\end{bmatrix} \\right\\| = 0$ dla losowych współrzędnych $v_1, v_2, v_3$ wektora $\\mathbf{v}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.rand(size=(3,))\n",
    "# TODO - begin\n",
    "x = ...\n",
    "print(x)\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] Z DŁUGOŚCIĄ WEKTORA Z PITAGORASA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operacje na wektorach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dodawanie i odejmowanie wektorów\n",
    "Dodawanie wektorów jest operacją, która polega na dodawaniu kolejnych współrzędnych wektorów. Dla przykładu, jeśli mamy dwa wektory $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ oraz $\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$ to suma tych wektorów to:\n",
    "$$ \\mathbf{v} + \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 1 + 3 \\\\ 2 + 4 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix} $$\n",
    "Odejmowanie wektorów działa analogicznie:\n",
    "$$ \\mathbf{v} - \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 1 - 3 \\\\ 2 - 4 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} $$\n",
    "\n",
    "Tego typu operacje mają sens geometryczny. Dodawanie wektorów to przesunięcie jednego wektora o drugi wektor. Odejmowanie wektorów to przesunięcie jednego wektora w przeciwnym kierunku drugiego wektora. Najprościej porównać to do naszego przemieszczania się po płaszczyźnie. Jeśli stoję w punkcie $(1, 2)$ i chcę przejść o $3$ w prawo i $4$ w górę to po prostu dodaję wektor $(3, 4)$ do wektora $(1, 2)$ i dostaję $(4, 6)$. Jeśli chcę wrócić do punktu $(1, 2)$ to odejmuję wektor $(3, 4)$ od wektora $(1, 2)$ i dostaję $(-2, -2)$.\n",
    "\n",
    "Wprowadźmy jeszcze intuicyjne pojęcie średniego wektora. Jeśli mamy $n$ wektorów $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n$ to średni wektor to:\n",
    "$$ \\mathbf{m} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{v}_i = \\begin{bmatrix} \\frac{1}{n} \\sum_{i=1}^n v_{i,1} \\\\ \\frac{1}{n} \\sum_{i=1}^n v_{i,2} \\\\ \\vdots \\\\ \\frac{1}{n} \\sum_{i=1}^n v_{i,d} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] Z PRZESUNIĘCIEM WEKTOR\n",
    "# [OBRAZEK] Z ŚREDNIM WEKTOREM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 6** Oblicz o jaką długość wektory $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$ oraz $\\mathbf{w} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}$ są oddalone od średniej $\\frac{\\mathbf{v} + \\mathbf{u} + \\mathbf{w}}{3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([1, 2])\n",
    "u = torch.tensor([3, 4])\n",
    "w = torch.tensor([5, 2])\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 7** Oblicz $\\mathbf{v}$ i $\\mathbf{u}$ dane równaniami $\\mathbf{v} + \\mathbf{u} = \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix}$ i $\\mathbf{v} - \\mathbf{u} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_plus_u = torch.tensor(data=[4, 6])\n",
    "v_minus_u = torch.tensor(data=[-2, -2])\n",
    "# TODO - begin\n",
    "v = ...\n",
    "u = ...\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnożenie wektora przez skalar\n",
    "Mnożenie wektora przez skalar to operacja, która polega na przemnożeniu każdej współrzędnej wektora przez tę samą liczbę nazywaną skalarem (*eng. scalar*). Dla przykładu, jeśli mamy wektor $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ oraz skalar $c = 2$ to iloczyn wektora przez skalar to:\n",
    "$$ c \\cdot \\mathbf{v} = 2 \\cdot \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 2 \\cdot 1 \\\\ 2 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} $$\n",
    "Mnożenie wektora przez skalar ma sens geometryczny. Mnożenie przez skalar $c$ to rozciągnięcie wektora $\\mathbf{v}$ o $c$ razy. Jeśli wektor $\\mathbf{v}$ reprezentuje przemieszczenie o $1$ w prawo i $2$ w górę to mnożenie przez $2$ da nam przemieszczenie o $2$ w prawo i $4$ w górę."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] Z ROZCIĄGNIĘCIEM WEKTORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako przykład weźmy operacje na wektorowej reprezentacji poczucia humoru, złośliwości i zamiłowania do wiosłowania osób zgromadzonych w jednym pomieszczeniu. Powiedzmy, że dla każdej osoby $i$ mamy wektor $\\mathbf{v}_i = (v_{i, 1}, v_{i, 2}, v_{i, 3})$, gdzie pierwsza współrzędna to poczucie humoru, druga to złośliwość, a trzecia to zamiłowanie do wiosłowania. Jeśli chcemy obliczyć średnie poczucie humoru, złośliwości i zamiłowania do wiosłowania w grupie osób to wystarczy obliczyć średnią arytmetyczną po wszystkich wektorach. Warto zauważyć, że operacje na wektorach mają sens tylko wtedy, gdy wektory mają tę samą liczbę współrzędnych. Otrzymamy wtedy:\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{v}_i = \\frac{1}{n} \\sum_{i=1}^{n} \\begin{bmatrix} v_{i, 1} \\\\ v_{i, 2} \\\\ v_{i, 3} \\end{bmatrix}$$\n",
    "Wspaniale, teraz mamy wspólną reprezentację dla grupy ludzi i możemy porównywać wektorowe reprezentacje dla powiedzmy różnych klas uczniów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 8** Zapisz wzór na modyfikację średniego wektora $\\mathbf{g}$ reprezentującego grupę 10 osób, jeśli:\n",
    "\n",
    "a) Nastała przerwa i chcemy zwiększyć złośliwość wszystkich osób o $3.14$.\n",
    "\n",
    "b) Klasowy błazen opowiedział anegdotę o człowieku, który zjadł wiosło i chcemy zwiększyć zamiłowanie do wiosłowania każdego ucznia stukrotnie. \n",
    "\n",
    "c) Z klasy wyszła osoba nauczycielska o wektorowej reprezentacji $v_1 = (v_{1, 1}, v_{1, 2}, v_{1, 3}) = (-44, 666, 997)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.tensor(data=[1, 2, 3])\n",
    "# TODO - begin\n",
    "# a)\n",
    "g_a = ...\n",
    "# b)\n",
    "g_b = ...\n",
    "# c)\n",
    "g_c = ...\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 9** Oblicz długosć wektora $\\mathbf{v} = (1, 2, 3, 4, 5)$ po przemnożeniu przez ustalony skalar dla parametrów `p=1` i `p=2` funkcji `torch.norm`. Jak zmienia się długość wektora po przemnożeniu przez skalar? Podaj wyjaśnienie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odległość między wektorami\n",
    "\n",
    "Odległość między dwoma wektorami $\\mathbf{v}$ i $\\mathbf{w}$ to nic innego jak długość wektora różnicy między nimi:\n",
    "$$ \\|\\mathbf{v} - \\mathbf{w}\\| = \\sqrt{(v_1 - w_1)^2 + (v_2 - w_2)^2 + \\ldots + (v_d - w_d)^2} $$\n",
    "co w przypadku dwuwymiarowym sprowadza się do:\n",
    "$$ \\|\\mathbf{v} - \\mathbf{w}\\| = \\sqrt{(v_1 - w_1)^2 + (v_2 - w_2)^2} $$\n",
    "\n",
    "Odległość między wektorami będziemy wykorzystywać do porównywania obserwacji ze zbiorów danych. Dla przykładu, jeżeli przez lata zbieraliśmy pozornie nieistotne informacje o preferencjach korzystania z internetu przez naszych znajomych, to możemy za pomocą sprytnych algorytmów stworzyć ich reprezentację wektorową. Otrzymamy wtedy prostą redukcję człowiek - wektor. Dzięki temu będziemy mogli łatwo porównywać naszych znajomych, szukać podobieństw i na przykład subtelnie manipulować ich zachowaniami proponując internetowe treści, które zaangażowały inne osoby  o podobnej reprezentacji w postaci słupa liczb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] Z ODLEGŁOŚCIĄ MIĘDZY WEKTORAMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 10** Zapisz układ równań pozwalający wyznaczyć wektor $\\mathbf{w}$ odległy od wektora $\\mathbf{v} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ o długości $1$ oraz odległy od wektora $\\mathbf{u} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ o długości $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iloczyn skalarny pary wektorów\n",
    "Iloczyn skalarny (*eng. dot product*) dwóch wektorów $\\mathbf{v}$ i $\\mathbf{w}$ to suma iloczynów odpowiadających sobie współrzędnych tych wektorów:\n",
    "$$ \\mathbf{v} \\cdot \\mathbf{w} = v_1 \\cdot w_1 + v_2 \\cdot w_2 + \\ldots + v_d \\cdot w_d $$\n",
    "Iloczyn skalarny będziemy również zapisywali jako $\\langle \\mathbf{v}, \\mathbf{w} \\rangle$. Zastanówmy się jak iloczyn skalarny ma się do podanego wcześniej wzoru na długość wektora:\n",
    "$$ \\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_d^2} = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 11** Oblicz iloczyn skalarny wektorów $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}$ oraz $\\mathbf{u} = \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}$ za pomocą funkcji `torch.dot`. Powinno wyjść $0$. Wektory, których iloczyn skalarny wynosi $0$ nazywamy prostopadłymi lub ortogonalnymi (*eng. orthogonal*). Znajdź wektor prostopadły jednocześnie do obu wektorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([1, 2, 0])\n",
    "u = torch.tensor([0, 1, 2])\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 12** Operacja `*` w pytorch to mnożenie element po elemencie (*eng. elementwise multiplication*). Znajdź sposób na obliczenie iloczynu skalarnego wektorów $\\mathbf{v}$ i $\\mathbf{w}$ za pomocą mnożenia element po elemencie i funkcji `torch.sum`. Uzupełnij elementy funkcji `iloczyn_skalarny` tak, aby zwracała iloczyn skalarny wektorów $\\mathbf{v}$ i $\\mathbf{w}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iloczyn_skalarny(v: Tensor, u: Tensor) -> float:\n",
    "    # TODO - begin\n",
    "    return ... # to co zwraca funkcja\n",
    "    # TODO - end\n",
    "\n",
    "def test_iloczyn_skalarny():\n",
    "    v = torch.rand(size=(3,))\n",
    "    u = torch.rand(size=(3,))\n",
    "    assert torch.allclose(iloczyn_skalarny(v, u),  torch.dot(v, u))\n",
    "    print(\"Test iloczyn_skalarny() zakończony sukcesem\")\n",
    "\n",
    "test_iloczyn_skalarny()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 13** Pokazać, że iloczyn skalarny jest liniowy, tzn. że dla dowolnych wektorów $\\mathbf{v}$, $\\mathbf{u}$ oraz skalarów $a$, $b$ zachodzi:\n",
    "$$ \\langle a \\mathbf{v} + b \\mathbf{u}, \\mathbf{w} \\rangle = a \\langle \\mathbf{v}, \\mathbf{w} \\rangle + b \\langle \\mathbf{u}, \\mathbf{w} \\rangle $$\n",
    "Zastanów się jak można to udowodnić korzystając z definicji iloczynu skalarnego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podobieństwo między wektorami\n",
    "Pojęcie podobieństwa możemy określać na podstawie odległości wektorów, ale popularnym pojęciem jest kosinus podobieństwa (*eng. cosine similarity*). Kosinus podobieństwo między dwoma wektorami $\\mathbf{v}$ i $\\mathbf{w}$ to iloczyn skalarny tych wektorów podzielony przez iloczyn długości tych wektorów:\n",
    "$$ \\cos(\\mathbf{v}, \\mathbf{w}) = \\text{cosine\\_similarity}(\\mathbf{v}, \\mathbf{w}) = \\frac{\\langle \\mathbf{v}, \\mathbf{w} \\rangle}{\\|\\mathbf{v}\\| \\cdot \\|\\mathbf{w}\\|} $$\n",
    "Kosinus podobieństwo przyjmuje wartości z przedziału $[-1, 1]$. Wartość $1$ oznacza, że wektory są identyczne, wartość $-1$ oznacza, że wektory są przeciwne, a wartość $0$ oznacza, że wektory są prostopadłe.\n",
    "\n",
    "Powyższy wzór określa kosinus kąta miedzy wektorami w przestrzeni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] Z PROSTOPADŁYMI WEKTORAMI I COS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 14** Pokazać, że iloczyn skalarny wektorów $\\mathbf{v}$ i $\\mathbf{w}$ jest równy iloczynowi długości wektorów $\\mathbf{v}$ i $\\mathbf{w}$ oraz cosinusa kąta między wektorami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 15** Pokazać związek powyższego wzoru na kosinus kąta między wektorami z tw. kosinusów: $c^2 = a^2 + b^2 - 2ab \\cos(\\alpha)$. Warto zwrócić uwagę, że iloczyn skalarny wektorów jak i norma wektora są po prostu liczbami rzeczywistymi tak samo jak $a$, $b$ czy $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 16** Pokaż, że iloczyn skalarny jest wrażliwy na skalowanie wektorów, a cosine similarity nie. Oblicz w `torch` przykład iloczynu skalarnego (`torch.dot`) i cosine similarityj (`torch.cosine_similarity`) dla wektorów $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ oraz $2 \\cdot \\mathbf{v} = 2 \\cdot \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 17** Znajdź wektor $\\mathbf{w}$, który jest najbardziej podobny do wektora $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, ale ma długość $1$. Sprawdź za pomocą funkcji `torch.cosine_similarity` czy faktycznie jest najbardziej podobny. Ustalanie długości wektora bez zmiany jego kierunku na $1$ to tzw. normalizacja wektora (*eng. vector normalization*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([1, 2], dtype=torch.float32) # jeżeli chcemy wykonywać operacje na liczbach zmiennoprzecinkowych, to musimy zadeklarować typ danych\n",
    "\n",
    "# TODO - begin\n",
    "w = ... # najlepiej zainicjalizować wektor w jako wynik pewnej operacji na v\n",
    "# TODO - end\n",
    "print(torch.cosine_similarity(v, w, dim=0)) # w funkcjach torch często trzeba podać wymiar, dla którego ma być wykonana operacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] WEKTOR O DLUGOSCI 1 O TYM SAMYM KIERUNKU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Macierze\n",
    "### Czym jest macierz?\n",
    "Macierz (*eng. matrix*) to uporządkowany zbiór liczb o określonej liczbie wierszy i kolumn. Można myśleć o macierzach jako tablicach dwuwymiarowych. Konkretnie macierz można zapisać jako:\n",
    "$$ \\mathbf{A} = \\begin{bmatrix} a_{1,1} & a_{1,2} & \\ldots & a_{1,m} \\\\ a_{2,1} & a_{2,2} & \\ldots & a_{2,m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & a_{n,2} & \\ldots & a_{n,m} \\end{bmatrix}$$\n",
    "gdzie $a_{i,j}$ to element macierzy $\\mathbf{A}$ znajdujący się na $i$-tym wierszu i $j$-tej kolumnie. Macierz $\\mathbf{A}$ ma $n$ wierszy i $m$ kolumn. Macierze będziemy wykorzystywać do reprezentowania zbiorów próbek czy obserwacji. Wiersze macierzy to wektory, które możemy traktować jako obserwacje, a kolumny to cechy tych obserwacji.\n",
    "\n",
    "W przypadku $n=4$ i $m=3$ macierz $\\mathbf{A}$ reprezentuje na przykład wyniki czterech bananów w testach na kwaśność, twardość i kolor. Wtedy $a_{1,1}$ to wynik kwaśności pierwszego banana w pierwszym teście, $a_{2,3}$ to wynik koloru drugiego banana w trzecim teście, a $a_{4,2}$ to wynik czwartego banana w teście na twardość (czyli nr 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] MACIERZ I BANANY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W pytorch macierz to tensor dwuwymiarowy. Możemy zainicjalizować macierz w następujący sposób:\n",
    "```python\n",
    "A = torch.tensor(\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    "    )\n",
    "```\n",
    "co krócej zapisujemy jako:\n",
    "```python\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "```\n",
    "\n",
    "A więc całkiem podobnie do inicjalizacji wektora z tym zagnieżdżamy listy w liście. W ten sposób otrzymujemy macierz o dwóch wierszach i trzech kolumnach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 18** Zainicjalizuj macierze o wymiarach $3 \\times 4$ oraz $1 \\times 3$ z losowymi wartościami za pomocą funkcji `torch.randn`. Wywołaj metodę `shape` na każdej z nich, aby sprawdzić ich wymiary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład - losowa macierz 3x3\n",
    "M = torch.rand(size=(3, 3))\n",
    "print(M.shape)\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 19** Dodaj na diagonali (*eng. diagonal*) (czyli na przekątnej począwszy od lewego górnego rogu na prawym dolnym skończywszy) macierzy $\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9\\end{bmatrix}$ liczbę $1$ i zapisz wynik w zmiennej `B`. Wywołaj metodę `diag` na macierzy `B`, aby sprawdzić wartości na przekątnej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(1, 10).reshape(3, 3)\n",
    "# Przykład - wybór elementu z macierzy\n",
    "A[:, 0] # pierwsza kolumna\n",
    "A[1, :] # drugi wiersz\n",
    "A[0, 0] # pierwszy element\n",
    "A[[0, 2], :] # pierwszy i trzeci wiersz\n",
    "A[[0, 1], [0, 2]] # pierwszy element pierwszego wiersza i trzeci element drugiego wiersza\n",
    "\n",
    "# TODO - begin\n",
    "B = ...\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 20** W `torch` tensory można indeksować ujemnymi liczbami w celu odwołania się do elementów od końca. Spróbuj odwołać się do elementu w drugim wierszu i trzeciej kolumnie macierzy `B` z poprzedniego zadania za pomocą indeksów ujemnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 21** Możemy zmieniać kształt macierzy za pomocą metody `view` lub `reshape`. Zmień kształt macierzy `B` z poprzedniego zadania na macierz o wymiarach $1 \\times 9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Przykład - zmiana kształtu macierzy\n",
    "A = torch.ones(size=(2, 3)) # inicjalizacja macierzy 2x3 z samymi jedynkami\n",
    "print(A)\n",
    "print(A.shape)\n",
    "print(A.reshape(3, 2)) # zwróć uwagę, że wykonanie reshape nie zmienia oryginalnej macierzy a zwraca nową\n",
    "print(A.reshape(3, 2).shape) # nowa macierz ma inny kształt\n",
    "\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operacje na macierzach\n",
    "### Mnożenie przez skalar, dodawanie i odejmowanie macierzy\n",
    "Mnożenie macierzy przez skalar, dodawanie i odejmowanie macierzy odbywa się analogicznie jak w przypadku wektorów. **Mnożenie macierzy przez skalar** to przemnożenie każdego elementu macierzy przez ten sam skalar. **Dodawanie i odejmowanie macierzy** to dodawanie i odejmowanie kolejnych elementów macierzy. Warto zwrócić uwagę, że operacje te mają sens tylko wtedy, gdy macierze mają takie same wymiary.\n",
    "\n",
    "Skoro możemy myśleć o macierzy jak o zbiorze wektorów, to mnożenie macierzy przez skalar to po prostu mnożenie każdego wektora w macierzy przez ten sam skalar. Dodawanie i odejmowanie macierzy to dodawanie i odejmowanie kolejnych wektorów w macierzy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 22** Dla danych macierzy $\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$ i $\\mathbf{B} = \\begin{bmatrix} 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{bmatrix}$. Znajdź macierz $\\mathbf{C} = 5.5 \\cdot \\mathbf{A} - 17 \\cdot \\mathbf{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "B = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# TODO - begin\n",
    "C = ...\n",
    "# TODO - end\n",
    "assert torch.allclose(C, torch.tensor([[-113.5000, -125.0000, -136.5000], [-148.0000, -159.5000, -171.0000]]), atol=1e-4), \"Ćoś inaczej powinno być\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 23** Jeżeli na wektor spojrzymy jak na zbiór wartości dla pojedynczej obserwacji (np. jednej papużki falistej) to na macierz spojrzymy jak na zbiór wartości dla wielu obserwacji (np. dla całego stadka papużek falistych). W pytorczu możliwe jest wygodne odejmowanie od macierzy wektora wierszowego lub kolumnowego. Dla macierzy opisującej stadko papużek falistych $\\mathbf{A}$ znajdź macierz, w której od każdego wiersza odjęty średni wektor opisujący hipotetyczną średnią papużkę o imieniu \"Kazik\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(size=(20, 6)) # mamy 20 papużek falistych i 6 cech tychże papużek\n",
    "\n",
    "# TODO - begin\n",
    "reprezentacja_Kazika = ...\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpozycja macierzy\n",
    "Transpozycja macierzy to operacja, która polega na zamianie miejscami wierszy i kolumn. Dla macierzy $\\mathbf{A}$ o wymiarach $n \\times m$ transpozycja macierzy $\\mathbf{A}$ to macierz $\\mathbf{A}^T$ o wymiarach $m \\times n$ taka, że:\n",
    "$$ \\mathbf{A}^T = \\underbrace{\\begin{bmatrix} a_{1,1} & a_{2,1} & \\ldots & a_{n,1} \\\\ a_{1,2} & a_{2,2} & \\ldots & a_{n,2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{1,m} & a_{2,m} & \\ldots & a_{n,m} \\end{bmatrix}}_{n \\text{ kolumn}}, \\text{ gdzie } \\mathbf{A} = \\underbrace{\\begin{bmatrix} a_{1,1} & a_{1,2} & \\ldots & a_{1,m} \\\\ a_{2,1} & a_{2,2} & \\ldots & a_{2,m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & a_{n,2} & \\ldots & a_{n,m} \\end{bmatrix}}_{m \\text{ kolumn}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W pytroch transpozycję macierzy możemy uzyskać korzystając z metody `t`, `T` funkcji `torch.transpose` lub metody `transpose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Przykład - transpozycja macierzy\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "A_t = A.t()\n",
    "A_T = A.T\n",
    "A_transpose = torch.transpose(A, 0, 1)\n",
    "A_transpose2 = A.transpose(0, 1)\n",
    "\n",
    "print(A)\n",
    "print(A_t)\n",
    "\n",
    "assert A_t.equal(A_T) and A_T.equal(A_transpose) and A_transpose.equal(A_transpose2), \"Coś nie tak z transpozycją\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnożenie wektora przez macierz\n",
    "Mnożenie macierzy przez wektor to operacja, w której każdy wiersz macierzy jest mnożony przez wektor. W ten sposób otrzymujemy nowy wektor. Mnożenie macierzy przez wektor ma sens tylko wtedy, gdy liczba kolumn macierzy jest równa liczbie współrzędnych wektora. Dla macierzy $\\mathbf{A}$ i wektora $\\mathbf{v}$ operację definiujemy jako:\n",
    "$$ \\mathbf{A} \\mathbf{v} = \\begin{bmatrix} a_{1,1} & a_{1,2} & \\ldots & a_{1,m} \\\\ a_{2,1} & a_{2,2} & \\ldots & a_{2,m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & a_{n,2} & \\ldots & a_{n,m} \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_m \\end{bmatrix} = \\begin{bmatrix} a_{1,1} v_1 + a_{1,2} v_2 + \\ldots + a_{1,m} v_m \\\\ a_{2,1} v_1 + a_{2,2} v_2 + \\ldots + a_{2,m} v_m \\\\ \\vdots \\\\ a_{n,1} v_1 + a_{n,2} v_2 + \\ldots + a_{n,m} v_m \\end{bmatrix} = \\begin{bmatrix} \\langle \\mathbf{a}_{1, :}, \\mathbf{v} \\rangle \\\\ \\langle \\mathbf{a}_{2, :}, \\mathbf{v} \\rangle \\\\ \\vdots \\\\   \\langle \\mathbf{a}_{n, :}, \\mathbf{v} \\rangle\\end{bmatrix}$$\n",
    "\n",
    "Po prostu dokonujemy iloczynu skalarnego każdego wiersza macierzy (oznaczenie $\\mathbf{a}_{i, :}$ dla $i$-tego wiersza) z wektorem, który mnożymy przez macierz.\n",
    "\n",
    "Na wynik mnożenia wektora przez macierz możemy spojrzeć jak na sumę ważoną kolumn macierzy (oznaczenie $\\mathbf{a}_{:, j}$ dla $j$-tego wiersza). Dla przykładu, jeśli mamy macierz $\\mathbf{A}$ o wymiarach $n \\times m$ i wektor $\\mathbf{v}$ o długości $m$, to wynik mnożenia $\\mathbf{A} \\mathbf{v}$ to wektor o długości $n$, gdzie każda współrzędna to suma ważona współrzędnych kolumn macierzy $\\mathbf{A}$.\n",
    "$$\\mathbf{A} \\mathbf{v} = \\begin{bmatrix} a_{1,1} & a_{1,2} & \\ldots & a_{1,m} \\\\ a_{2,1} & a_{2,2} & \\ldots & a_{2,m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & a_{n,2} & \\ldots & a_{n,m} \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_m \\end{bmatrix} = v_1 \\cdot \\mathbf{a}_{:, 1} + v_2 \\cdot \\mathbf{a}_{:, 2} + \\dots + v_m \\cdot \\mathbf{a}_{:, m} $$\n",
    "\n",
    "Warto zauważyć, że mnożenie wektora przez macierz to operacja liniowa. Oznacza to, że suma dwóch mnożeń wektora przez macierz to to samo co mnożenie sumy dwóch wektorów przez macierz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 24** Pokazać, że zachodzi $\\mathbf{A} (\\mathbf{v} + \\mathbf{u}) = \\mathbf{A} \\mathbf{v} + \\mathbf{A} \\mathbf{u}$ dla dowolnych macierzy $\\mathbf{A}$ i wektorów $\\mathbf{v}$, $\\mathbf{u}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] MNOŻENIE WEKTORA PRZEZ MACIERZ JAKO WAŻONA SUMA KOLUMN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W pytorch mnożenie macierzy przez wektor realizujemy za pomocą funkcji `torch.matmul` lub operatora `@`. Przykładowo:\n",
    "```python\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "v = torch.tensor([1, 2, 3])\n",
    "torch.matmul(A, v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 25** Za pomocą mnożenia wektora przez macierz znajdź wartości wektora $\\mathbf{c} = 2 \\cdot \\mathbf{a} + 6 \\cdot \\mathbf{b}$, gdzie $\\mathbf{a}$ i $\\mathbf{b}$ stanowią wektory kolumnowe macierzy $\\mathbf{X} = [\\mathbf{a} \\, \\mathbf{b}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3]).reshape(3, 1) # macierz 3x1, czyli wektor kolumnowy\n",
    "b = torch.tensor([4, 5, 6]).reshape(3, 1) # macierz 3x1, czyli wektor kolumnowy\n",
    "X = torch.cat((a, b), dim=1) # konkatenacja macierzy a i b wzdłuż drugiej osi czyli kolumnowo\n",
    "print(a.shape, b.shape, X.shape)\n",
    "del a, b # usuwamy zmienne a i b, bo nie są już potrzebne\n",
    "\n",
    "# TODO - begin\n",
    "c = ...\n",
    "# TODO - end\n",
    "\n",
    "print(c)\n",
    "assert torch.allclose(c, torch.tensor([26, 34, 42])), \"Coś nie tak z obliczeniami\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 26** Jest dana macierz danych $\\mathbf{X}_{5 \\times 4}$ o $5$ wierszach - obserwacjach oraz $4$ kolumnach - cechach. Za pomocą mnożenia macierzy przez ustalony przez siebie wektor oraz transpozycji macierzy wyznacz średni wektor określający średnią wartość każdej cechy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[34, 56, 23, 90],\n",
    "                  [12, 45, 67, 89],\n",
    "                  [45, 67, 89, 12],\n",
    "                  [23, 45, 67, 89],\n",
    "                  [12, 34, 56, 78]])\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnożenie wektora przez macierz jako transformacja wektora\n",
    "Jak wyżej pokazaliśmy na mnożenie wektora przez macierz można patrzeć jak na wybieranie kolumn macierzy z odpowiednimi wagami. W ten sposób mnożenie wektora przez macierz to transformacja wektora. Aby, złapać lepszą intuicję przyjżyjmy się transformacjom danym przez szczególne macierze.\n",
    "\n",
    "#### Skalowanie współrzędnych wektora\n",
    "Zacznijmy od macierzy diagonalnej (*eng. diagonal matrix*). Macierz diagonalna to macierz kwadratowa (*eng. square matrix*) - mająca tyle samo kolumn co wierszy, której elementy poza przekątną są równe $0$. Dla przykładu macierz diagonalna $\\mathbf{D}$ o wymiarach $2 \\times 2$ to:\n",
    "$$ \\mathbf{D} = \\begin{bmatrix} d_{1,1} & 0 \\\\ 0 & d_{2,2} \\end{bmatrix} $$\n",
    "gdzie $d_{1,1}$ i $d_{2,2}$ to elementy na przekątnej macierzy. Mnożenie wektora przez macierz diagonalną to po prostu mnożenie każdego elementu wektora przez odpowiadający mu element na przekątnej macierzy. W ten sposób możemy skalować współrzędne wektora. Dla konkretnego wektora $\\mathbf{v}$ i macierzy diagonalnej $\\mathbf{D}$ mamy:\n",
    "$$ \\mathbf{D} \\mathbf{v} = \\begin{bmatrix} d_{1,1} & 0 \\\\ 0 & d_{2,2} \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} d_{1,1} \\cdot v_1 + 0 \\cdot v_2\\\\ 0 \\cdot v_1 + d_{2,2} \\cdot v_2 \\end{bmatrix} = \\begin{bmatrix} d_{1,1} v_1\\\\ d_{2,2} v_2 \\end{bmatrix} $$\n",
    "Warto jednak myśleć o mnożeniu wektora przez macierz w ogólności jako wybieraniu kolumn macierzy z odpowiednimi wagami. W przypadku macierzy diagonalnej wagi dane przez wektor napotykają na zera w macierzy i w konsekwencji otrzymujemy skalowanie wektora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 27** Dla danego wektora $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ znajdź macierz diagonalną $\\mathbf{D}$, która skaluje współrzędne wektora do wartości $2$ i $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([1, 2])\n",
    "\n",
    "# TODO - begin\n",
    "D = torch.tensor([[..., ...], [..., ...]])\n",
    "# TODO - end\n",
    "print(D @ v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 28** Jest dana macierz diagonalna $\\mathbf{D}$ o wymiarach $3 \\times 3$ o elementach na przekątnej równe $1.7$, $1$ i $0.4$. Napisz pętę, która będzie kolejno przemnażała podany wektor $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$ przez macierz $\\mathbf{D}$, zastępowała go wynikiem tej operacji i wyświetlała wynik w kolejnych $10$ iteracjach. Sprawdź wynik dla innego wektora. Skomentuj jak zmienia się wektor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.diag(torch.tensor([1.7, 1, 0.4]))\n",
    "v = torch.ones(size=(3, ))\n",
    "# TODO - begin\n",
    "for i in range(10): # 10 iteracji pętli\n",
    "    v = ... # kod wykonywany w pętli\n",
    "    print(v)\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 29** Powtórz ćwiczenie 27, ale tym razem po każdej iteracji znormalizuj wektor $\\mathbf{v}$ do długości $1$ przed wypisaniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.diag(torch.tensor([1.7, 1, 0.4]))\n",
    "v = torch.ones(size=(3, ))\n",
    "# TODO - begin\n",
    "for i in range(10):\n",
    "    ...\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 30** Powtórz ćwiczenie 28 z macierzą $\\mathbf{D} = \\begin{bmatrix} 0.3 & 0 & 0 \\\\ 0 & 0.8 & 0 \\\\ 0 & 0 & 0.1 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.diag(torch.tensor([.3, .8, .1]))\n",
    "v = torch.ones(size=(3, ))\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrót wektora\n",
    "Kolejnym przykładem transformacji wektora za pomocą macierzy jest obrót wektora. Przyjżymy się wyłącznie przypadkowi dwuwymiarowemu. Obrót wektora o kąt $\\theta$ wokół początku układu współrzędnych możemy zrealizować za pomocą macierzy obrotu $\\mathbf{R}$:\n",
    "$$ \\mathbf{R} = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} $$\n",
    "gdzie $\\cos(\\theta)$ i $\\sin(\\theta)$ to funkcje trygonometryczne.\n",
    "\n",
    "Aby lepiej zrozumieć skąd taka postać macierzy obrotu zobaczmy jak wygląda mnożenie wektora jednostkowego $\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ przez macierz $\\mathbf{R}$:\n",
    "$$ \\mathbf{R} \\mathbf{e}_1 = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\cos(\\theta) \\\\ \\sin(\\theta) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] OBRÓT WEKTORA [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przeprowadźmy tę samą transformację dla wektora jednostkowego $\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$:\n",
    "$$ \\mathbf{R} \\mathbf{e}_2 = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\sin(\\theta) \\\\ \\cos(\\theta) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] OBRÓT WEKTORA [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy teraz, że zgodnie ze wzorem na kosinus kąta między wektorami:\n",
    "$$ \\cos(\\mathbf{e}_1, \\mathbf{e}_2) = \\frac{\\langle \\mathbf{e}_1, \\mathbf{e}_2 \\rangle}{\\|\\mathbf{e}_1\\| \\cdot \\|\\mathbf{e}_2\\|} = 0 $$\n",
    "co oznacza, że wektory $\\mathbf{e}_1$ i $\\mathbf{e}_2$ są prostopadłe. Oraz dla $\\mathbf{e}_1$ i $\\mathbf{e}_2$ zachodzi:\n",
    "$$ \\cos(\\mathbf{e}_i, \\mathbf{R} \\mathbf{e}_i) = \\frac{\\langle \\mathbf{e}_i, \\mathbf{R} \\mathbf{e}_i \\rangle}{\\|\\mathbf{e}_i\\| \\cdot \\|\\mathbf{R} \\mathbf{e}_i\\|} = \\cos(\\theta), \\text{ dla } i=1, 2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 31** Sprawdzić powyższe obserwacje. Na podstawie tych obserwacji oraz liniowości iloczynu skalarnego uzasadnić, że macierz obrotu $\\mathbf{R}$ obraca dowolny wektor $\\mathbf{v}$ o kąt $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 32** Twojemu ziomalowi udało się shakować autonomiczną kosiarkę nieznośnego sąsiada. Na ten moment macie dostęp jedynie do interfejsu sterowania, który określa kierunek jazdy kosiarki za pomocą dwuwymiarowego wektora $\\mathbf{v}$, który określa przemieszczenie podczas jednego ruchu autonomicznej kosiarki. Dokładniej rzecz ujmując kosiarka najpierw orientuje się w kierunku wektora a następnie jedzie o jego długość do przodu. Sąsiad już się zbliża i będzie okazja zobaczyć jego skwaszoną minę. Musicie w dwóch ruchach skosić zbyt ładne petunie sąsiada. Podaj dwa wektory, które pozwolą wykonać to zadanie dla przypadku opisanego na obrazku. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] ZDALNA KONTROLA KOSIARKI - kosiarka ma pojechać o dwa prosto . skręcić o 30 stopni w lewo i pojechać o 1 prosto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# napisać kotrolę kosiarki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnożenie macierzy przez macierz\n",
    "Mnożenie macierzy przez macierz to operacja, w której każdy wiersz pierwszej macierzy jest mnożony przez każdą kolumnę drugiej macierzy. W ten sposób otrzymujemy nową macierz. Mnożenie macierzy przez macierz ma sens tylko wtedy, gdy liczba kolumn pierwszej macierzy jest równa liczbie wierszy drugiej macierzy. Dla macierzy $\\mathbf{A}$ i $\\mathbf{B}$ operację definiujemy jako:\n",
    "$$ \\mathbf{A} \\mathbf{B} = \\begin{bmatrix} a_{1,1} & a_{1,2} & \\ldots & a_{1,m} \\\\ a_{2,1} & a_{2,2} & \\ldots & a_{2,m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & a_{n,2} & \\ldots & a_{n,m} \\end{bmatrix} \\begin{bmatrix} b_{1,1} & b_{1,2} & \\ldots & b_{1,p} \\\\ b_{2,1} & b_{2,2} & \\ldots & b_{2,p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{m,1} & b_{m,2} & \\ldots & b_{m,p} \\end{bmatrix} = \\begin{bmatrix} a_{1,1} b_{1,1} + a_{1,2} b_{2,1} + \\ldots + a_{1,m} b_{m,1} & \\ldots & a_{1,1} b_{1,p} + a_{1,2} b_{2,p} + \\ldots + a_{1,m} b_{m,p} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n,1} b_{1,1} + a_{n,2} b_{2,1} + \\ldots + a_{n,m} b_{m,1} & \\ldots & a_{n,1} b_{1,p} + a_{n,2} b_{2,p} + \\ldots + a_{n,m} b_{m,p} \\end{bmatrix} $$\n",
    "\n",
    "Można powiedzieć, że teraz w każdej kolumnie wynikowej macierzy umieszczamy wynik mnożenia wektora kolumnowego drugiej macierzy w pierwszą macierzą. Zapiszmy to jeszcze z użyciem iloczynu skalarnego podobnie jak w przypadku mnożenia wektora przez macierz:\n",
    "$$ \\mathbf{A} \\mathbf{B} = \\begin{bmatrix} \\langle \\mathbf{a}_{1, :}, \\mathbf{b}_{:, 1} \\rangle & \\ldots & \\langle \\mathbf{a}_{1, :}, \\mathbf{b}_{:, p} \\rangle \\\\ \\vdots & \\ddots & \\vdots \\\\ \\langle \\mathbf{a}_{n, :}, \\mathbf{b}_{:, 1} \\rangle & \\ldots & \\langle \\mathbf{a}_{n, :}, \\mathbf{b}_{:, p} \\rangle \\end{bmatrix} $$\n",
    "\n",
    "Wygląda na to, że na mnożenie macierzy przez macierz możemy patrzeć jak na mnożenie wektorów kolumnowych drugiej macierzy przez pierwszą macierz.\n",
    "\n",
    "Spójrzmy jeszcze na prostszy przykład dla macierzy $2 \\times 2$. Mnożenie macierzy $\\mathbf{A} = \\begin{bmatrix} a_{1,1} & a_{1,2} \\\\ a_{2,1} & a_{2,2} \\end{bmatrix}$ i $\\mathbf{B} = \\begin{bmatrix} b_{1,1} & b_{1,2} \\\\ b_{2,1} & b_{2,2} \\end{bmatrix}$ to:\n",
    "$$ \\mathbf{A} \\mathbf{B} = \\begin{bmatrix} a_{1,1} b_{1,1} + a_{1,2} b_{2,1} & a_{1,1} b_{1,2} + a_{1,2} b_{2,2} \\\\ a_{2,1} b_{1,1} + a_{2,2} b_{2,1} & a_{2,1} b_{1,2} + a_{2,2} b_{2,2} \\end{bmatrix}$$\n",
    "\n",
    "Kluczowe jest, aby zastanowić się czy mnożenie macierzy przez macierz ma sens dla macierzy o danych wymiarach.\n",
    "\n",
    "Warto zwrócić uwagę, że mnożenie macierzy nie jest przemienne, tzn. $\\mathbf{A} \\mathbf{B} \\neq \\mathbf{B} \\mathbf{A}$. Co więcej dla odwrotnej kolejności macierzy $\\mathbf{B} \\mathbf{A}$ mnożenie może nie mieć sensu jeżeli wymiary się nie zgadzają."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Na wynik mnożenia wektora przez macierz możemy spojrzeć jak na sumę ważoną kolumn macierzy (oznaczenie $\\mathbf{a}_{:, j}$ dla $j$-tego wiersza). Dla przykładu, jeśli mamy macierz $\\mathbf{A}$ o wymiarach $n \\times m$ i wektor $\\mathbf{v}$ o długości $m$, to wynik mnożenia $\\mathbf{A} \\mathbf{v}$ to wektor o długości $n$, gdzie każda współrzędna to suma ważona współrzędnych kolumn macierzy $\\mathbf{A}$. -->\n",
    "Skorzystajmy z zapisu mnożenia wektora przez macierz jako ważonej sumy kolumn macierzy. Jako, że z definicji mnożenia macierzy $j$-ta kolumna wyniku $\\mathbf{A} \\mathbf{B}$ zależy wyłącznie od $j$-tej kolumny $\\mathbf{B}$, to możemy zapisać wynik mnożenia macierzy przez macierz jako sumę ważoną kolumn macierzy $\\mathbf{A}$, gdzie wagi to kolumny macierzy $\\mathbf{B}$. Dostajemy wtedy następujący wzór dla $j$-tej kolumny wyniku $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$:\n",
    "\n",
    "$$\\mathbf{c}_{:, j} = \\mathbf{A} \\mathbf{b}_{:, j} = \\begin{bmatrix} a_{1,1} & a_{1,2} & \\ldots & a_{1,m} \\\\ a_{2,1} & a_{2,2} & \\ldots & a_{2,m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & a_{n,2} & \\ldots & a_{n,m} \\end{bmatrix} \\begin{bmatrix} b_{1, j} \\\\ b_{2, j} \\\\ \\vdots \\\\ b_{m, j} \\end{bmatrix} = \\underbrace{b_{1, j} \\cdot \\mathbf{a}_{:, 1}}_{\\text{kolumna }\\mathbf{A} \\times \\text{waga}} + b_{2, j} \\cdot \\mathbf{a}_{:, 2} + \\dots + b_{m, j} \\cdot \\mathbf{a}_{:, m}$$\n",
    "\n",
    "Przyjżyjmy się teraz udziałowi każdej z kolumn macierzy $\\mathbf{A}$ w kolumnach $\\mathbf{C}$. Pierwsza kolumna $\\mathbf{a}_{:, j}$ macierzy $\\mathbf{A}$ jest dodawana w kolejnych kolumnach macierzy $\\mathbf{C}$ z wagami $b_{1, 1}, b_{1, 2}, \\dots, b_{1, p}$. Co oznacza, że wpływ pierwszej kolumny macierzy $\\mathbf{A}$ jest zależny wyłącznie od pierwszego wiersza macierzy $\\mathbf{B}$. Analogicznie druga kolumna $\\mathbf{a}_{:, 2}$ macierzy $\\mathbf{A}$ jest dodawana w kolejnych kolumnach macierzy $\\mathbf{C}$ z wagami $b_{2, 1}, b_{2, 2}, \\dots, b_{2, p}$. \n",
    "\n",
    "W ten sposób możemy zapisać inną formułę mnożenia macierzy przez macierz korzystając z kolumn macierzy $\\mathbf{A}$ i wierszy macierzy $\\mathbf{B}$:\n",
    "$$\\mathbf{C}_{n \\times p} = \\mathbf{A}_{n \\times m} \\mathbf{B}_{m \\times p} = \\begin{bmatrix} \\mathbf{a}_{:, 1} & \\mathbf{a}_{:, 2} & \\ldots & \\mathbf{a}_{:, m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b}_{:, 1} \\\\ \\mathbf{b}_{:, 2} \\\\ \\vdots \\\\ \\mathbf{b}_{:, p} \\end{bmatrix} = \\underbrace{\\mathbf{a}_{:, 1} \\mathbf{b}_{:, 1}}_{n \\times p} + \\mathbf{a}_{:, 2} \\mathbf{b}_{:, 2} + \\dots + \\mathbf{a}_{:, m} \\mathbf{b}_{:, p}$$\n",
    "W ten sposób mnożenie macierzy można zapisać jako sumę macierzy utworzonych z iloczynów kolumn macierzy $\\mathbf{A}$ i wierszy macierzy $\\mathbf{B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] POPULARNY ZAPIS MNOŻENIA MACIERZY PRZEZ MACIERZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W pytorch mnożenie macierzy przez macierz realizujemy za pomocą funkcji `torch.matmul` lub operatora `@`. Przykładowo:\n",
    "```python\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "B = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
    "torch.matmul(A, B)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 33** Znajdź macierz $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$ dla danych macierzy $\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6\\end{bmatrix}$ i $\\mathbf{B} = \\begin{bmatrix} 7 & 8 \\\\ 9 & 10 \\end{bmatrix}$. Sprawdź co się stanie jeżeli spróbujemy przemnożyć $\\mathbf{A}$ przez $\\mathbf{B}$, a co gdy $\\mathbf{A^T}$ przez $\\mathbf{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "B = torch.tensor([[7, 8], [9, 10]])\n",
    "# TODO - begin\n",
    "C = ...\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 34** Roważmy funkcję $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$ zadaną wzorem $f(\\mathbf{x}) = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1$, gdzie $\\mathbf{W}_1 = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}$ i $\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$ oraz funkcję $g: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^2$ zadaną wzorem $g(\\mathbf{x}) = \\mathbf{W}_2 \\mathbf{x} + \\mathbf{b}_2$, gdzie $\\mathbf{W}_2 = \\begin{bmatrix} 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{bmatrix}$ i $\\mathbf{b}_2 = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}$. Znajdź funkcję $h: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ zdefiniowaną jako $h = g \\circ f$ czyli złożenie $f$ i $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "b1 = torch.ones(size=(3, 1))\n",
    "W2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "b2 = torch.fill_(torch.empty(size=(2, 1)), 2)\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensory\n",
    "Tensory stanowią są ogólniejsze od macierzy i pozwalają na określenie dowolnej liczby wymiarów. W przypadku macierzy mówiliśmy o wymiarach $n \\times m$, gdzie $n$ to liczba wierszy, a $m$ to liczba kolumn. W przypadku tensorów mówimy o wymiarach $n_1 \\times n_2 \\times \\ldots \\times n_k$, gdzie $n_1, n_2, \\ldots, n_k$ to liczba współrzędnych w każdym z wymiarów.\n",
    "\n",
    "Nie będziemy stosować wymyślnych operacji na tensorach a ogramniczmy się do traktowania ich jak zbiorów macierzy.\n",
    "\n",
    "Zwróćmy uwagę na drobną niezręczność. Mówimy np. że wektor $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^3$ ma trzy wymiary a nie jeden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W pytorch tensor tworzymy za pomocą znanej funkcji `torch.tensor` nie ograniczając się do 1 czy 2 wymiarów przy definicji. Przykładowo:\n",
    "```python\n",
    "torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "torch.randn(3, 4, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OBRAZEK] TENSOR Z WYMIARAMI 2x3x4, można dodawać więcej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 35** Zdefiniuj losowy tensor $\\mathbf{X}$ o wymiarach $2 \\times 3 \\times 4$ i tensor $\\mathbf{Y}$ o wymiarach $2 \\times 4 \\times 5$. Wywołaj na nim funkcję `torch.matmul`. Wynik będzie miał wymiary $2 \\times 3 \\times 5$. Co się jednak dzieje przy wywołaniu tej funkcji? Tensor $\\mathbf{X}$ składa się z dwóch macierzy $3 \\times 4$, a tensor $\\mathbf{Y}$ z dwóch macierzy $4 \\times 5$. Wywyłując funkcję `torch.matmul` mnożymy każdą z macierzy z $\\mathbf{X}$ przez odpowiadającą jej macierz z $\\mathbf{Y}$. Tak samo będzie dla większej liczby wymiarów. Wystarczy, że ostatni wymiar pierwszego tensora będzie się zgadzał z przedostatnim drugiego co jest wymagane do przemnożenia macierzy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - begin\n",
    "X = torch.randn(size=(...))\n",
    "Y = torch.randn(size=(...))\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 36** Spróbuj przemnożyć tensor o wymiarach $2\\times 5\\times 7 \\times 2$ przez tensor o wymiarach $2 \\times 9$. Sprawdź wymiary otrzymanego wyniku i zastanów się skąd się one wzięły."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 7, 9])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(2, 5, 7, 2))\n",
    "Y = torch.rand(size=(2, 9))\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wygodne pytorchowe operacje na tensorach\n",
    "Pytorch dostarcza mechanizm tzw. rozgłaszania (*eng. broadcasting*), który pozwala na wygodne operacje na tensorach o różnych wymiarach. Rozgłaszanie polega na automatycznym powieleniu tensora o mniejszych wymiarach tak, aby można było wykonać operację z tensorem o większych wymiarach. Przykładowo, jeżeli mamy tensor $\\mathbf{A}$ o wymiarach $2 \\times 3$ i tensor $\\mathbf{B}$ o wymiarach $3$, to możemy wykonać operację dodawania między nimi. Pytorch automatycznie powieli tensor $\\mathbf{B}$ tak, aby mógł być dodany do każdego wiersza tensora $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 37** Zdefiniuj tensor $\\mathbf{A}$ o wymiarach $2 \\times 3$, tensor $\\mathbf{B}$ o wymiarach $3$ oraz tensor $\\mathbf{C}$ o wymiarach $1 \\times 3$. Dodaj tensor $\\mathbf{B}$ do każdego wiersza tensora $\\mathbf{A}$. Dodaj tensor $\\mathbf{C}$ do każdej kolumny tensora $\\mathbf{A}$. Wypisz wyniki obu obliczeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.zeros(2, 3)\n",
    "B = torch.tensor([1, 2, 3])\n",
    "C = torch.tensor([[4, 5]])\n",
    "\n",
    "# TODO - begin\n",
    "\n",
    "# TODO - end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
